{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers.models.bert.modeling_bert import BertIntermediate, BertOutput, BertAttention\n",
    "\n",
    "BertLayerNorm = nn.LayerNorm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T17:27:35.736651545Z",
     "start_time": "2023-10-30T17:27:35.687925465Z"
    }
   },
   "id": "e1e47e7bd504d2d3"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.crossattention = BertAttention(config)\n",
    "        # BertIntermediate: linear + gelu\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        # BertOutput: layer+dropout+residual+layer_norm\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask\n",
    "    ):\n",
    "        self_attention_outputs = self.attention(hidden_states)\n",
    "        # print(\"the shape of hidden_states: \", hidden_states.shape)\n",
    "        # print(\"the shape of self_attention_outputs: \", self_attention_outputs[0].shape)\n",
    "\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "        if encoder_attention_mask.dim() == 3:\n",
    "            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
    "        elif encoder_attention_mask.dim() == 2:\n",
    "            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n",
    "                    encoder_hidden_shape, encoder_attention_mask.shape\n",
    "                )\n",
    "            )\n",
    "        # The mask is also transformed so that positions with a value of 1 (indicating they should be masked) are set to\n",
    "        # a large negative value (-10000.0), making them effectively zero when passed through a softmax.\n",
    "        \n",
    "        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -10000.0\n",
    "\n",
    "        \n",
    "        cross_attention_outputs = self.crossattention(\n",
    "            hidden_states=attention_output, encoder_hidden_states=encoder_hidden_states,  encoder_attention_mask=encoder_extended_attention_mask\n",
    "        )\n",
    "        attention_output = cross_attention_outputs[0]\n",
    "        # print(\"the shape of cross_attention_outputs: \", cross_attention_outputs[0].shape)\n",
    "        outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T19:05:11.100145130Z",
     "start_time": "2023-10-31T19:05:11.058027041Z"
    }
   },
   "id": "7925edb8253a3f79"
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "class SetDecoder_regressive_output(nn.Module):\n",
    "    def __init__(self, config, num_generated_triples, num_layers, num_classes, return_intermediate=False, use_ILP=False):\n",
    "        super().__init__()\n",
    "        self.return_intermediate = return_intermediate\n",
    "        self.num_generated_triples = num_generated_triples\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(num_layers)])\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.query_embed = nn.Embedding(num_generated_triples, config.hidden_size)\n",
    "        if use_ILP:\n",
    "            self.decoder2class = nn.Linear(config.hidden_size, num_classes)\n",
    "            self.class2hidden = nn.Linear(num_classes, config.hidden_size)\n",
    "        else:\n",
    "            self.decoder2class = nn.Linear(config.hidden_size, num_classes + 1)\n",
    "            self.class2hidden = nn.Linear(num_classes + 1, config.hidden_size)\n",
    "        # self.decoder2span = nn.Linear(config.hidden_size, 4)\n",
    "\n",
    "        self.head_start_metric_1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.head_end_metric_1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.tail_start_metric_1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.tail_end_metric_1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.head_start_metric_2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.head_end_metric_2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.tail_start_metric_2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.tail_end_metric_2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        torch.nn.init.orthogonal_(self.head_start_metric_1.weight, gain=1)\n",
    "        torch.nn.init.orthogonal_(self.head_end_metric_1.weight, gain=1)\n",
    "        torch.nn.init.orthogonal_(self.tail_start_metric_1.weight, gain=1)\n",
    "        torch.nn.init.orthogonal_(self.tail_end_metric_1.weight, gain=1)\n",
    "        torch.nn.init.orthogonal_(self.head_start_metric_2.weight, gain=1)\n",
    "        torch.nn.init.orthogonal_(self.head_end_metric_2.weight, gain=1)\n",
    "        torch.nn.init.orthogonal_(self.tail_start_metric_2.weight, gain=1)\n",
    "        torch.nn.init.orthogonal_(self.tail_end_metric_2.weight, gain=1)\n",
    "        torch.nn.init.orthogonal_(self.query_embed.weight, gain=1)\n",
    "        \n",
    "        self.regressive_decoder = nn.MultiheadAttention(embed_dim=config.hidden_size, num_heads=1, batch_first=True)\n",
    "        self.output_linear = nn.Linear(config.hidden_size * 4, 4, bias=False)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, encoder_hidden_states, encoder_attention_mask):\n",
    "        bsz = encoder_hidden_states.size()[0]\n",
    "        # print(\"the shape of query_embed.weight: \", self.query_embed.weight.shape)\n",
    "        hidden_states = self.query_embed.weight.unsqueeze(0).repeat(bsz, 1, 1)\n",
    "        # print(\"the shape of hidden_states: \", hidden_states.shape)\n",
    "        # hidden_state: [bsz, num_generated_triples, hidden_size]\n",
    "        hidden_states = self.dropout(self.LayerNorm(hidden_states))\n",
    "        all_hidden_states = ()\n",
    "        \n",
    "        print(\"=====================================\")\n",
    "        # print(f\"encoder_attention_mask:\\n{encoder_attention_mask}\")\n",
    "                \n",
    "        for i, layer_module in enumerate(self.layers):\n",
    "            # print(f\"hidden_states shape:\\n{hidden_states.shape}\")\n",
    "            # print(f\"encoder_hidden_states shape:\\n{encoder_hidden_states.shape}\")\n",
    "            # print(\"=====================================\")\n",
    "            \n",
    "            if self.return_intermediate:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states, encoder_hidden_states, encoder_attention_mask\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "        class_logits = self.decoder2class(hidden_states)\n",
    "        \n",
    "        class_hidden_states = self.class2hidden(class_logits)\n",
    "        # class_hidden_states = class_hidden_states.unsqueeze(-2).repeat(1, 1, encoder_hidden_states.shape[-2], 1)\n",
    "        # print(f\"class_hidden_states after repeat shape:\\n{class_hidden_states.shape}\")\n",
    "        # have a binary tensor encorder_extended_attention_mask_binary, shape is the same as encoder_extended_attention_mask, and the value is True for 0, and False for 1.\n",
    "        if encoder_attention_mask.dim() == 3:\n",
    "            encoder_extended_attention_mask = encoder_attention_mask.reshape(-1, encoder_hidden_states.shape[-2])\n",
    "        elif encoder_attention_mask.dim() == 2:\n",
    "            encoder_extended_attention_mask = encoder_attention_mask\n",
    "        encoder_extended_attention_mask_binary = (encoder_extended_attention_mask == 0)\n",
    "        # print(f\"encoder_extended_attention_mask_binary: \\n{encoder_extended_attention_mask_binary}\")\n",
    "        # print(f\"encoder_extended_attention_mask_binary shape: \\n{encoder_extended_attention_mask_binary.shape}\")\n",
    "        \n",
    "        encoder_extended_attention_mask_binary =  encoder_extended_attention_mask_binary.unsqueeze(1).repeat(1, self.num_generated_triples, 1)\n",
    "        # print(f\"encoder_extended_attention_mask_binary shape: \\n{encoder_extended_attention_mask_binary.shape}\")\n",
    "        \n",
    "        encoder_extended_attention_mask_binary = encoder_extended_attention_mask_binary.reshape(bsz * self.num_generated_triples, encoder_hidden_states.shape[-2])\n",
    "        # print(f\"encoder_extended_attention_mask_binary shape: \\n{encoder_extended_attention_mask_binary.shape}\")\n",
    "        \n",
    "        # print(f\"encoder_hidden_states shape:\\n{encoder_hidden_states.shape}\")\n",
    "        encoder_hidden_states = encoder_hidden_states.repeat(self.num_generated_triples, 1, 1)\n",
    "        # print(f\"encoder_hidden_states shape after:\\n{encoder_hidden_states.shape}\")\n",
    "        \n",
    "        # print(f\"encoder_extended_attention_mask_binary shape:\\n{encoder_extended_attention_mask_binary.shape}\")\n",
    "        \n",
    "        \n",
    "        class_hidden_states = class_hidden_states.reshape(-1, class_hidden_states.shape[-1]).unsqueeze(1)\n",
    "        # print(f\"class_hidden_states shape after:\\n{class_hidden_states.shape}\")\n",
    "        \n",
    "        hidden_states = hidden_states.reshape(-1, hidden_states.shape[-1]).unsqueeze(1)\n",
    "        # print(f\"hidden_states shape after:\\n{hidden_states.shape}\")\n",
    "        \n",
    "        # print(f\"encoder_extended_attention_mask_binary.unsqueeze(-1) shape:\\n{encoder_extended_attention_mask_binary.unsqueeze(-1).shape}\")\n",
    "        head_start_logits_mh = self.regressive_decoder(query=encoder_hidden_states, key=torch.tanh(self.head_start_metric_1(class_hidden_states) + self.head_start_metric_2(hidden_states)), value=torch.tanh(self.head_start_metric_1(class_hidden_states) + self.head_start_metric_2(hidden_states)), attn_mask=encoder_extended_attention_mask_binary.unsqueeze(-1))\n",
    "        head_start_logits_mh = head_start_logits_mh[0]\n",
    "        # print(f\"head_start_logits_mh shape:\\n{head_start_logits_mh.shape}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        head_end_logits_mh = self.regressive_decoder(head_start_logits_mh, key=torch.tanh(self.head_end_metric_1(class_hidden_states) + self.head_end_metric_2(hidden_states)), value=torch.tanh(self.head_end_metric_1(class_hidden_states) + self.head_end_metric_2(hidden_states)), attn_mask=encoder_extended_attention_mask_binary.unsqueeze(-1))\n",
    "        head_end_logits_mh = head_end_logits_mh[0]\n",
    "                \n",
    "        # print(f\"head_end_logits_mh shape:\\n{head_end_logits_mh.shape}\")\n",
    "\n",
    "\n",
    "        tail_start_logits_mh = self.regressive_decoder(head_end_logits_mh, key=torch.tanh(self.tail_start_metric_1(class_hidden_states) + self.tail_start_metric_2(hidden_states)), value=torch.tanh(self.tail_start_metric_1(class_hidden_states) + self.tail_start_metric_2(hidden_states)), attn_mask=encoder_extended_attention_mask_binary.unsqueeze(-1))\n",
    "        tail_start_logits_mh = tail_start_logits_mh[0]\n",
    "        \n",
    "        # print(f\"tail_start_logits_mh shape:\\n{tail_start_logits_mh.shape}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        tail_end_logits_mh = self.regressive_decoder(tail_start_logits_mh, key=torch.tanh(self.tail_end_metric_1(class_hidden_states) + self.tail_end_metric_2(hidden_states)), value=torch.tanh(self.tail_end_metric_1(class_hidden_states) + self.tail_end_metric_2(hidden_states)), attn_mask=encoder_extended_attention_mask_binary.unsqueeze(-1))\n",
    "        tail_end_logits_mh = tail_end_logits_mh[0]\n",
    "        \n",
    "        # print(f\"tail_end_logits_mh shape:\\n{tail_end_logits_mh.shape}\")\n",
    "        # print(\"=============linear==================\")\n",
    "        # Stack the four tensors along a new dimension in-place\n",
    "        input_tensor = torch.stack([head_start_logits_mh, head_end_logits_mh, tail_start_logits_mh, tail_end_logits_mh], dim=3)\n",
    "        \n",
    "        # Flatten the input tensor along the last dimension\n",
    "        input_tensor_flat = input_tensor.view(input_tensor.size(0), input_tensor.size(1), -1)\n",
    "        # print(f\"input_tensor_flat shape:\\n{input_tensor_flat.shape}\")\n",
    "        \n",
    "        # Apply the linear layer\n",
    "        output_flat = self.output_linear(input_tensor_flat)\n",
    "        # print(f\"output_flat shape:\\n{output_flat.shape}\")\n",
    "        \n",
    "        # Reshape the output tensor to separate it into four tensors\n",
    "        output = output_flat.view(bsz, self.num_generated_triples, output_flat.size(1), 4)\n",
    "\n",
    "        # Split the output tensor in-place into the four individual tensors\n",
    "        head_start_logits  = output[:, :, :, 0].clone()\n",
    "        head_end_logits = output[:, :, :, 1].clone()\n",
    "        tail_start_logits = output[:, :, :, 2].clone()\n",
    "        tail_end_logits = output[:, :, :, 3].clone()\n",
    "\n",
    "        return class_logits, head_start_logits, head_end_logits, tail_start_logits, tail_end_logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T12:13:43.645816354Z",
     "start_time": "2023-11-01T12:13:43.553167457Z"
    }
   },
   "id": "267c6c5003b0af50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10fa55777720361f"
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "from models.seq_encoder import SeqEncoder\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        # self.bert_directory = \"bert-base-cased\"\n",
    "        self.bert_directory = \"SpanBERT/spanbert-base-cased\"\n",
    "        self.fix_bert_embeddings = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T12:15:06.345977771Z",
     "start_time": "2023-11-01T12:15:06.247304318Z"
    }
   },
   "id": "68ce5be19b00374b"
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state:  torch.Size([2, 18, 768])\n",
      "pooler_output:  torch.Size([2, 768]) \n",
      "====================\n",
      "\n",
      "=====================================\n",
      "==================crossattention===================\n",
      "class_hidden_states after repeat shape:\n",
      "torch.Size([2, 5, 768])\n",
      "encoder_extended_attention_mask_binary: \n",
      "tensor([[False, False, False, False, False, False, False, False,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False]])\n",
      "encoder_extended_attention_mask_binary shape: \n",
      "torch.Size([2, 18])\n",
      "encoder_extended_attention_mask_binary shape: \n",
      "torch.Size([2, 5, 18])\n",
      "encoder_extended_attention_mask_binary shape: \n",
      "torch.Size([10, 18])\n",
      "encoder_hidden_states shape:\n",
      "torch.Size([2, 18, 768])\n",
      "encoder_hidden_states shape after:\n",
      "torch.Size([10, 18, 768])\n",
      "encoder_extended_attention_mask_binary shape:\n",
      "torch.Size([10, 18])\n",
      "class_hidden_states shape after:\n",
      "torch.Size([10, 1, 768])\n",
      "hidden_states shape after:\n",
      "torch.Size([10, 1, 768])\n",
      "encoder_extended_attention_mask_binary.unsqueeze(-1) shape:\n",
      "torch.Size([10, 18, 1])\n",
      "head_start_logits_mh shape:\n",
      "torch.Size([10, 18, 768])\n",
      "head_end_logits_mh shape:\n",
      "torch.Size([10, 18, 768])\n",
      "tail_start_logits_mh shape:\n",
      "torch.Size([10, 18, 768])\n",
      "tail_end_logits_mh shape:\n",
      "torch.Size([10, 18, 768])\n",
      "=============linear==================\n",
      "input_tensor_flat shape:\n",
      "torch.Size([10, 18, 3072])\n",
      "output_flat shape:\n",
      "torch.Size([10, 18, 4])\n",
      "head_start_logits shape: torch.Size([2, 5, 18])\n",
      "tokenized_text[\"attention_mask\"] shape: torch.Size([2, 18])\n",
      "class_logits:  torch.Size([2, 5, 11])\n",
      "head_start_logits:  torch.Size([2, 5, 18])\n",
      "head_end_logits:  torch.Size([2, 5, 18])\n",
      "tail_start_logits:  torch.Size([2, 5, 18])\n",
      "tail_end_logits:  torch.Size([2, 5, 18])\n",
      "class_logits_argmax:  tensor([[ 3, 10,  2,  6,  9],\n",
      "        [ 3,  1,  2,  5,  3]])\n"
     ]
    }
   ],
   "source": [
    "bert_args = Args()\n",
    "\n",
    "encoder = SeqEncoder(bert_args)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_args.bert_directory)\n",
    "\n",
    "decoder = SetDecoder(encoder.config, 5, 2, 10, return_intermediate=False)\n",
    "\n",
    "text = [\"Hello, my dog is cute\", \"Do you want to play with me? I think we have some commons.\"]\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "last_hidden_state, pooler_output = encoder(input_ids=tokenized_text[\"input_ids\"], attention_mask=tokenized_text[\"attention_mask\"])\n",
    "\n",
    "print(\"last_hidden_state: \", last_hidden_state.shape)\n",
    "print(\"pooler_output: \", pooler_output.shape, \"\\n====================\\n\")\n",
    "\n",
    "\n",
    "decoder_output = decoder(encoder_hidden_states=last_hidden_state, encoder_attention_mask=tokenized_text[\"attention_mask\"])\n",
    "class_logits, head_start_logits, head_end_logits, tail_start_logits, tail_end_logits  = decoder_output\n",
    "\n",
    "print(f\"head_start_logits shape: {head_start_logits.shape}\")\n",
    "print(f'tokenized_text[\"attention_mask\"] shape: {tokenized_text[\"attention_mask\"].shape}')\n",
    "head_start_logits = head_start_logits.squeeze(-1).masked_fill((1 - tokenized_text[\"attention_mask\"].unsqueeze(1)).bool(), -10000.0)\n",
    "head_end_logits = head_end_logits.squeeze(-1).masked_fill((1 - tokenized_text[\"attention_mask\"].unsqueeze(1)).bool(), -10000.0)\n",
    "tail_start_logits = tail_start_logits.squeeze(-1).masked_fill((1 - tokenized_text[\"attention_mask\"].unsqueeze(1)).bool(), -10000.0)\n",
    "tail_end_logits = tail_end_logits.squeeze(-1).masked_fill((1 - tokenized_text[\"attention_mask\"].unsqueeze(1)).bool(), -10000.0)# [bsz, num_generated_triples, seq_len]\n",
    "\n",
    "print(\"class_logits: \", class_logits.shape)\n",
    "print(\"head_start_logits: \", head_start_logits.shape)\n",
    "print(\"head_end_logits: \", head_end_logits.shape)\n",
    "print(\"tail_start_logits: \", tail_start_logits.shape)\n",
    "print(\"tail_end_logits: \", tail_end_logits.shape)\n",
    "\n",
    "# have the argmax of the class_logits\n",
    "class_logits_argmax = class_logits.argmax(-1)\n",
    "print(\"class_logits_argmax: \", class_logits_argmax)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T12:15:12.183949521Z",
     "start_time": "2023-11-01T12:15:07.832527710Z"
    }
   },
   "id": "e7dae03a1936cbf4"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "Linear(in_features=768, out_features=1, bias=False)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.head_start_metric_3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T18:56:11.066984311Z",
     "start_time": "2023-10-31T18:56:10.980617036Z"
    }
   },
   "id": "fa77189e3c61ec91"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
